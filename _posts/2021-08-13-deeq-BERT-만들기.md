---
layout: post
title:  "deeqnlp로 BERT 만들기"
subtitle: "deeqnlp 토크나이저를 이용한 BERT 학습"
date:  2021-08-20 12:00:00 +0900
background: '/img/post-001.jpg'
categories:
   - NLP 
tags:
   - BERT
   - gpu
   - python
   - baikalai
   - 바이칼AI
   - 자연어처리
   - 인공지능
   - natural language processing
   - 바이칼에이아이


author: hunbl <hunbl@baikal.ai>
---

# deeqnlp+BERT?

...
오리지널 BERT는 Wordpiece라는 방식으로 vocab(토큰뭉치)을 만듭니다. 말뭉치(corpus)에 있는 모든 단어, 단어조각(단어를 잘 자르면 토큰수를 줄일 수 있죠)을 집계해서 빈도수가 높은 토큰들을 모아 vocab을 만드는 방법입니다. 영어는 단어=어절이라서 잘린 단어를 붙이기만하면 쉽게 의미단위(원래의 단어)로 복원할 수 있으니 문제가 없지만, 한글에 적용하면 만들어진 토큰들이 의미단위와 상관없는 말조각이 된다는 문제가 생깁니다. (예를들어 빈도수만 따지면 '나', '나는', 나를' 이 따로따로 토큰으로 생성될 수 있습니다. 한국어를 잘 이해하는 vocab이 되려면, 저 3개의 토큰은 '나', '는', '를' 3개의 토큰이 되는게 더 나아 보입니다.) 다행히 BERT의 토크나이저를 다른걸로 바꾸는건 전혀 어려운일이 아니고, 또 다행2 우리는 한글을 의미단위로 잘 나누어주는 deeqnlp라는 형태소분석기를 가지고 있습니다. 어차피 BERT를 학습시킬거면 deeqnlp 형태소분석기를 사용한 토크나이저를 쓰는게 좋겠네요. 
...

# CORPUS - 말뭉치

BERT의 사전학습에 필요한 기본적인 학습데이터는 
1. 일단 당연히 양이 많아야 됩니다.
2. 문법, 어법에 맞는 완결된 문장으로 구성되어야 합니다.
3. 의미에 방해가 되는 요소들은 되도록 배제해야 합니다.
4. 문단 단위로 잘 나뉘어져 있어야 합니다 (이건 BERT류에만 해당)

- 1,2를 위해 일단 양이 많고 잘 정제된 말뭉치를 선택합니다. 국립국어원의 문어, 뉴스와 한국어 위키를 모아보니 잘 정제된 내용들로 16억 단어가 넘는 말뭉치를 만들 수 있습니다.
- 2,3을 위해 문장이 너무 길거나(길면 문장이 중간에 잘려서 의미를 알 수 없게 됩니다) 너무 짧으면(문장이 한 두 단어라면 제대로 의미를 가지는 문장이 아닐 수 있으므로 제외시킬 수 있습니다) 뺍니다.
- 3을 위해 email, url, 전화번호 등 의미를 알 수 없는 또는 표현할 필요없는 내용을 스크리닝할 필요도 있습니다.
- 4를 위해 문단과 문단 사이가 빈 줄로 잘 나뉠 수 있도록 처리해야 합니다. 

말뭉치의 원소스들을 위 조건에 맞게 텍스트파일들로 만들어두면 corpus 준비끝입니다.

# vocab - 토큰뭉치

문제의 vocab입니다. 우리는 2개의 vocab과 2개의 모델을 만들 예정입니다. 우선 오리지널 Wordpiece 토크나이저로 만든 vocab입니다. 이걸로 만든 모델은 BERT를 사용하는 모든 상황에 거의 아무것도 안 바꾸고 그대로 사용할 수 있다는 장점이 있습니다. 두번째는 우리의 주인공 deeqnlp 형태소분석기로 만든 vocab과 모델입이다. 모델을 만드는 과정은 비슷하지만 만들어진 모델을 사용하려면 사용하는 코드에 deeqnlp 토크나이저를 추가해서 수정을 해 줘야 합니다. 

- Wordpiece vocab: 오리지널 BERT에는 학습할 수 있는 코드가 없지만 huggingface의 tokenizers 라이브러리를 사용하면 위에서 준비한 코퍼스를 사용하여 쉽게 vocab을 만들 수 있습니다.
(code)

- deeqnlp vocab: 말뭉치의 모든 문장을 deeqnlp로 형태소분석하고 빈도수에 따른 원시 토큰 목록을 만듭니다. 문제는 형태소분석으로 만든 토큰은 너무 다양해서 빈도수로 자르면, 빈도수로 만들어낸 Wordpiece같은 vocab에 비해 토큰을 못 찾는(unknown token) 비율이 너무 커지는 문제가 있습니다. 이 문제를 해결하기위해 deeqnlp tokenizer는 말뭉치에 포함된 음절을 vocab에 포함시키고 형태소분석 토큰 상위권에 포함되지않는 어절(unknown이 될 애들)을 음절 분리하여 토크나이징하는 방법을 사용합니다.

# 학습데이터 생성

말뭉치를 학습을 할 수 있는 데이터로 바꿔야 합니다.

# 학습

이제 필요한건 돈입니다.

# finetune

쓸만한가요? NO

