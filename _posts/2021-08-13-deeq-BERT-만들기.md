---
layout: post
title:  "deeqnlp로 BERT 만들기"
subtitle: "deeqnlp 토크나이저를 이용한 BERT 학습"
date:  2021-08-20 12:00:00 +0900
background: '/img/post-001.jpg'
categories:
   - NLP 
tags:
   - BERT
   - gpu
   - python
   - baikalai
   - 바이칼AI
   - 자연어처리
   - 인공지능
   - natural language processing
   - 바이칼에이아이


author: hunbl <hunbl@baikal.ai>
---

# BERT

BERT(**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)를 만들어야 하는 첫번째 이유는 얘가 자연어처리를 아주 잘하기 때문입니다. BERT로 사전학습모델(Pre-trained model)을 한 번 만들어 놓으면 이것 하나만으로 쉽게 다양한 자연어처리 문제를 풀 수 있습니다. 이전까지는 자연어처리 문제마다 다 다른 복잡한 모델이 많이 존재했고 각각 다 다른 방법으로 학습을 시켜야 했거든요. 지금은 BERT보다 낫다는 여러 후계자들에게 1등 자리를 내준것 같기도 하지만, 여전히 BERT는 현역이고 NLP의 기본이기때문에 우리는 일단 BERT의 사전학습모댈을 만들어 보기로 했습니다. 그런데말입니다 그렇게 널리 사용되는 모델이라면 이미 많은 사전학습모델이 공개되어 있지는 않을까요? 이 오픈소스의 시대에? ...네 물론 많이 있습니다. 하지만 우리에겐 아직도 BERT를 만들어야하는 몇가지 이유가 있었습니다. 우선 우리의 목적은 원래의 BERT가 사용하는 토크나이저를 우리 deeqnlp 형태소 분석으로 바꾸는 것이었습니다. BERT에 기본 탑재된 토크나이저는 인접된 문자들의 출현 빈도만 고려해서 토큰을 만들기때문에 단어들이 기본 분리되어 있는 영어같은 언어에는 잘 맞지만 의미단위 어절이 붙어있는 경우가 많은 우리나라말에 적용하면 말이 안되는(의미를 반영하지 않는) 토큰들이 잔뜩 나오게 됩니다. 그래서 우리는 토크나이저에 형태소분석을 사용해서 한글을 잘 이해하는 BERT를 만들고자 했던 것입니다. 또 하나의 이유는 BERT 모델을 0부터 만들 수 있는 경험과 기술이 다양한 목적의 사전학습모델을 만들거나 다른 모델의 사전학습모델도 만들 수 있는 바탕이 될거라 생각했기 때문입니다.
(그림?)

# CORPUS - 말뭉치

위에서 사전학습모델만 만들어두면 만사 OK인것처럼 넘어갔지만 사실은 여기 함정이 있습니다. '사전학습'의 목적이 이 세상 모든 문서를 다 읽어서 마치 언어를 이해하는 것같은 모델을 만드는 것에 있다는 겁니다. 즉 사전학습모델을 만들기 위해서는 엄청나게 많은 말뭉치가 필요합니다. 원조 BERT가 총 30억어절의 문서를 3~4번 학습했다고 하죠. 우리도 보통 수백만에서 수천만 줄의 문서를 준비해서 학습을 여러번 시켜야 합니다. 우선은 그만한 문서가 있다고 치고 학습에 맞는 문서는 어떤것이고 어떻게 준비(업계 용어로 정제라고 합니다)해야 하는지 알아봅니다.
- 우선 문법, 어법에 맞고 보통 많이 쓰이는 완결된 문장부터 학습을 시키는 것이 좋습니다. 책, 뉴스, 사전 등이 좋겠죠. 양은 최소 수백만 줄은 넘어야 하고 다양한 내용이 섞여 있을수록 좋은것 같습니다.
- 이런 원본 말뭉치들이 준비되었으면 학습데이터를 만들 수 있는 기본 말뭉치로 만듭니다. 기본 말뭉치는 text파일이고 **한 줄에 한 문장**, **문단과 문단사이는 빈 문장** 의 법칙만 지키면 됩니다!
- 이것을 위해서 준비된 원본의 포맷에 따라 text, csv, json, xml 리더 등을 만들어야 합니다.
- 문장분리, 특수 문자 제거 등의 전처리를 해야 합니다.
- 불용어, 의미가 없는 문장 삭제 등의 처리를 해야합니다. 의미 없는 내용이 포함되면 학습에 방해가 되기때문에 중요한데 이것은 소스에 따라 완전 케바케이므로 사용하려는 원본을 잘 보고 적절한 전처리를 만들어야 합니다.
- 예를들어 문장이 한단어(또는 두 단어 이하?)이면 제목이거나 의미를 알 수 없는 내용일 것이므로 삭제. 이메일, 전화번호, url등이 빈번하게 등장하는 소스라면 모두 불용어 처리해야 합니다. 이모지, 한자 등의 삭제. 반복되는 문자 삭제. 반복되는 불용어 문장(이건 뉴스, 게시판 스크래핑 데이터에 포함될 수 있는 광고, 명령 라인 등 여러 경우가 있을 수 있습니다)에 대한 패턴검색 삭제 등 엄청 다양합니다.
(그림)

# vocab - 토큰뭉치

문제의 vocab입니다. 우리는 2개의 vocab과 2개의 모델을 만들 예정입니다. 우선 오리지널 Wordpiece 토크나이저로 만든 vocab입니다. 이걸로 만든 모델은 BERT를 사용하는 모든 상황에 거의 아무것도 안 바꾸고 그대로 사용할 수 있다는 장점이 있습니다. 두번째는 우리의 주인공 deeqnlp 형태소분석기로 만든 vocab과 모델입이다. 모델을 만드는 과정은 비슷하지만 만들어진 모델을 사용하려면 사용하는 코드에 deeqnlp 토크나이저를 추가해서 수정을 해 줘야 합니다. 

- Wordpiece vocab: 오리지널 BERT에는 학습할 수 있는 코드가 없지만 huggingface의 tokenizers 라이브러리를 사용하면 위에서 준비한 코퍼스를 사용하여 쉽게 vocab을 만들 수 있습니다.
(code)

- deeqnlp vocab: 말뭉치의 모든 문장을 deeqnlp로 형태소분석하고 빈도수에 따른 원시 토큰 목록을 만듭니다. 문제는 형태소분석으로 만든 토큰은 너무 다양해서 빈도수로 자르면, 빈도수로 만들어낸 Wordpiece같은 vocab에 비해 토큰을 못 찾는(unknown token) 비율이 너무 커지는 문제가 있습니다. 이 문제를 해결하기위해 deeqnlp tokenizer는 말뭉치에 포함된 음절을 vocab에 포함시키고 형태소분석 토큰 상위권에 포함되지않는 어절(unknown이 될 애들)을 음절 분리하여 토크나이징하는 방법을 사용합니다.

# 학습데이터 생성

말뭉치를 학습을 할 수 있는 데이터로 바꿔야 합니다.

# 학습

이제 필요한건 돈입니다.

# finetune

쓸만한가요? NO

