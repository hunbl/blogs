---
layout: post
title:  "deeqnlp로 BERT 만들기"
subtitle: "deeqnlp 토크나이저를 이용한 BERT 학습"
date:  2021-08-20 12:00:00 +0900
background: '/img/post-001.jpg'
categories:
   - NLP 
tags:
   - BERT
   - gpu
   - python
   - baikalai
   - 바이칼AI
   - 자연어처리
   - 인공지능
   - natural language processing
   - 바이칼에이아이


author: hunbl <hunbl@baikal.ai>
---

# BERT

BERT(**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)를 만들어야 하는 첫번째 이유는 얘가 자연어처리를 아주 잘하기 때문입니다. 
BERT로 사전학습모델(Pre-trained model)을 한 번 만들어 놓으면, 이것 하나만으로 쉽게 다양한 자연어처리 문제를 풀 수 있습니다. 
이전까지는 자연어처리는 문제 종류마다 각기 다른 복잡한 모델들이 존재했고 다 따로따로 학습을 시켜야 했거든요. 
지금은 BERT보다 낫다는 여러 후계자들에게 1등 자리를 내준것 같기도 하지만, 여전히 BERT는 현역이고 NLP의 기본이기때문에 우리는 일단 BERT의 사전학습모댈을 만들어 보기로 합니다.
그런데말입니다 그렇게 널리 사용되는 모델이라면 이미 많은 사전학습모델이 공개되어 있지는 않을까요? 이 오픈소스의 시대에? ...네 물론 많이 있습니다. 하지만 우리에겐 아직도 BERT를 새로 학습시켜야하는 몇가지 이유가 있습니다. 
우선 우리의 목적은 원래의 BERT가 사용하는 토크나이저를 우리 deeqnlp 형태소 분석으로 바꾸는 것이었습니다. BERT에 기본 탑재된 토크나이저는 인접된 문자들의 출현 빈도만 고려해서 토큰을 만들기때문에 단어들이 기본 분리되어 있는 영어같은 언어에는 잘 맞지만, 의미단위 어절이 붙어있는 경우가 많은 우리나라말에 적용하면 말이 안되는(의미를 반영하지 않는) 토큰들이 잔뜩 나오게 됩니다. 그래서 우리는 토크나이저에 형태소분석을 사용해서 한글을 잘 이해하는 BERT를 만들고자 했던 것입니다. 
또 하나의 이유는 BERT 모델을 0으로부터 만들 수 있는 경험과 기술이 다양한 목적의 NLP 모델을 만들거나 다른 모델의 사전학습모델도 만들 수 있는 바탕이 될거라 생각했기 때문입니다.

# CORPUS - 말뭉치

위에서 사전학습모델만 만들어두면 만사 OK인것처럼 넘어갔지만 사실은 여긴 함정이 있습니다. '사전학습'의 목적이 **이 세상 모든 문서**를 다 읽혀서 마치 언어를 이해하는 것같은 모델을 만드는 것에 있다는 겁니다. 즉 사전학습모델을 만들기 위해서는 엄청나게 많은 말뭉치가 필요합니다. 원조 BERT가 총 30억어절의 문서 전체를 3~4번 학습했다고 하죠. 우리도 보통 수백만에서 수천만 줄의 문서를 준비해서 무지 많은 학습을 시켜야 합니다. 그럼 이런 학습을 위한 원본 문서는 어때야 하고 어떤 가공처리공정(이쪽 업계에선 정제라고 합니다)을 거쳐야 하는지 알아봅시다.
- 우선 문법, 어법에 맞고 보통 많이 쓰이는 완결된 문장부터 학습을 시키는 것이 좋습니다. 책, 뉴스, 사전 등이 좋겠죠. 양은 최소 수백만 줄은 넘어야 하고 다양한 내용이 섞여 있을수록 좋은것 같습니다.
- 이런 원본 말뭉치들이 준비되었으면 학습데이터를 만들 수 있는 기본 말뭉치로 만듭니다. 기본 말뭉치는 text파일이고 **한 줄에 한 문장**, **문단과 문단사이는 빈 문장** 의 법칙만 지키면 됩니다!
- 이것을 위해서 준비된 원본의 포맷에 따라 text, csv, json, xml 리더 등을 만들어야 합니다.
- 문장분리, 특수 문자 제거 등의 전처리를 해야 합니다.
- 불용어, 의미가 없는 문장 삭제 등의 처리를 해야합니다. 의미 없는 내용이 포함되면 학습에 방해가 되기때문에 중요한데 이것은 소스에 따라 완전 케바케이므로 사용하려는 원본을 잘 보고 적절한 전처리를 만들어야 합니다.
- 예를들어 문장이 한단어(또는 두 단어 이하?)이면 제목이거나 의미를 알 수 없는 내용일 것이므로 삭제. 이메일, 전화번호, url등이 빈번하게 등장하는 소스라면 모두 불용어 처리해야 합니다. 이모지, 한자 등의 삭제. 반복되는 문자 삭제. 반복되는 불용어 문장(이건 뉴스, 게시판 스크래핑 데이터에 포함될 수 있는 광고, 명령 라인 등 여러 경우가 있을 수 있습니다)에 대한 패턴검색 삭제 등 엄청 다양합니다.

# vocab & Tokenizer

vocab은 중요합니다. 자연어를 모델의 입력이 될 수 있는 숫자로 바꾸는 기준이 되는 사전이 vocab, 바꾸는 방법이 토크나이저라고 할 수 있습니다. 아무튼 일단 vocab을 만들면 vocab을 기준으로 모델이 만들어지고 이후 모델과 vocab은 같이 다녀야합니다. vocab이 없어지면 모델은 아무 의미없는 숫자덩어리입니다. 우리는 두가지 vocab을 만들고 두가지 모델을 만들겁니다. 하나는 기본 토크나이저(BERT)로 만든 vocab과 모델, 두번째는 deeqnlp로 만든 토크나이저와 vocab, 그리고 모델입니다. 기본 토크나이저에 의한 vocab과 모델은 모델 성능 평가의 기준점이 되기도하고 다른 사람들이 사용하기에, 또 우리가 여러가지 태스크에 사용하기도 좋은 기준 모델이 됩니다. BERT를 사용하는 모든 곳에 우리 모델 바이너리만(물론 당연히 vocab도) 배포하면 사용할 수 있기 때문입니다. 두번째 모델은 사용하기가 간단하지 않을 수 있습니다. 우리의 사전학습모델과 함께 토크나이저의 변경이 필요하기때문입니다. (소스블럭)

# 학습데이터 생성

드디어 BERT 코드가 등장합니다. 코드는 원래 google 코드에 텐서플로우 2.x 업그레이드만 적용했고 deeqnlp 토크나이저 부분만 추가했습니다.

```
python bert/create_pretraining_data.py \
  --input_file= "/data/corpus/train-00001.txt" \
  --vocab_file="vocab.txt" \
  --tokenizer="deeq" \
  --max_seq_length=128 \
  --do_lower_case=False \
  --dupe_factor=1 \
  --output_file="/data/bert/train-00001.tf"
```
- tokenizer: deeqnlp 토크나이저를 사용하기위해 추가한 옵션입니다. 주어지지않으면 원래 토크나이저를 사용합니다.
- max_seq_length: 원래 BERT는 512입니다만 이 크기로 학습하면 GPU 메모리를 너무 많이 사용하기때문에 일단 128로 학습시킵니다. 문장의 토큰이 최대 128개라는 의미라서 웬만한 문장은 다 커버됩니다. 그리고 우선 128로 학습시킨다음에 512크기의 데이터로 추가학습을 시키면 512크기의 모델을 만들 수 있다고 합니다.
- do_lower_case: 우리는 영어를 사용할게 아니라서 대소문자 변환하지 않습니다. lowercase=False는 vocab만들때, 토크나이저 여기저기, finetue 설정 등 각종 config에 많이 등장하므로 앞으로도 등장할때마다 잘 맞추어주어야 합니다.
- dupe_factor: 학습데이터 수가 부족하면 dupe_factor=9 처럼 데이터를 늘일 수 있습니다. 이것은 입력 문장 하나로 몇개의 학습데이터를 만들지를 정하는 옵션입니다. 데이터를 늘이면 학습 정확도가 높게 나오긴하는데 실제 모델 성능이 높아지는 것 같지는 않습니다. 비슷한 문제를 여러번 푸는셈이니까 점수는 잘 나온다는 거겠죠. 경험상 무조건 **많고 다양한** 학습데이터가 좋은 모델을 만듭니다.

이렇게 위에서 정제한 모든 학습파일을 텐서플로우의 학습파일로 만들어주면 학습할 준비가 끝납니다.

# 학습

이제는 돈($)과 시간($$)이 사용될 차례입니다.

```
python bert/run_pretraining.py \
  --bert_config_file="bert_config.json" \
  --input_file="/data/bert/train*.tf" \
  --output_dir="/data/out/bert/" \
  --do_train=True \
  --do_eval=True \
  --max_seq_length=128 \
  --train_batch_size=124 \
  --num_train_steps=1000000 \
  --gpus="1" \
```
- max_seq_length: 학습데이터 만들때와 같습니다.
- train_batch_size: GPU(혹은 TPU)가 혀용하는 최대값을 해주는게 좋습니다. 학습시간 문제만이 아니고 배치크기가 클수록 학습이 더 잘된다고 합니다. 32G GPU 1개를 쓴다면 128 정도 크기가 가능합니다.
- num_train_steps: 학습데이터 크기에 따라 다르겠죠. 원래 BERT는 러프하게 30억 어절을 512배치로 1백만 스텝 돌려서 3epoch 정도 학습했다고 하는데 학습데이터와 배치사이즈가 다 다르겠지만 일단은 저정도 크기에 맞추서 최종 학습 목표를 잡으면 됩니다. 대체적으로 학습데이터의 갯수와 배치사이즈로 step수를 계산할 수 있고 이것을 바탕으로 최종 학습 횟수를 정하면 될것 같습니다.


# 잘되고 있습니까?

체크포인트를 정해서 여러가지 NLU 작업을 시켜보고 학습이 잘 되고 있는지 봅니다.

| model / task | question-pair | hate-speech | korsts | nsmc | kornli | 참고 |
| ---- | ---- | --- | --- | --- | --- | --- |
| bert-multi | 92.7 | 56.6 | 77.6 | 86.4 | 75.8 | 구글 다국어 BERT, 비교군 |
| wordpiece 550k | 93.4 | 64.8 | 78.6 | 86.6 | 75.5 | BERT vocab, 550000 steps 학습 |
| wordpiece 1M | **94.3** | 61.8 | 79.5 | 86.7 | 76.1 | BERT vocab, 1000000 steps 학습 |
| wordpiece 1.5M | 93.8 | 63.2 | **80.4** | 86.6 | **76.8** | BERT vocab, 1500000 steps 학습 |
| deeqnlp 660k | 90.4 | **64.4** | 43.2 | **87.8** | 71.1 | deeqnlp, 600000 steps 학습 |
| deeqnlp 1M | 90.5 | 60.8 | 49.0 | 87.6 | 70.4 | deeqnlp, 1000000 steps 학습 |
| deeqnlp 1.5M | 90.6 | 63.7 | 50.6 | 87.7 | 70.4 | deeqnlp, 1500000 steps 학습 |

- bert-multi 말고는 아직 학습중인 상태입니다.
- deeqnlp는 vocab사이즈가 커서 성능향상이 더딥니다. 앞으로 학습을 더 많이 시켜야 합니다.
- 모델마다 잘하는 태스크가 조금씩 다릅니다. 또 태스크 점수는 점수일뿐 성능의 절대적인 지표가 아닙니다. 
- 아무튼 대충 잘 되고 있는것 같습니다.

이상으로 실제 BERT를 처음부터 모델까지 학습시키는 과정에 대한 경험과 실전적인 내용을 대충 훑어봤습니다. 
