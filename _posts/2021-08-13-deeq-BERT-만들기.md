---
layout: post
title:  "deeqnlp로 BERT 만들기"
subtitle: "deeqnlp 토크나이저를 이용한 BERT 학습"
date:  2021-08-20 12:00:00 +0900
background: '/img/post-001.jpg'
categories:
   - NLP 
tags:
   - BERT
   - gpu
   - python
   - baikalai
   - 바이칼AI
   - 자연어처리
   - 인공지능
   - natural language processing
   - 바이칼에이아이


author: hunbl <hunbl@baikal.ai>
---

# BERT

BERT(**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)를 만들어야 하는 첫번째 이유는 얘가 자연어처리를 아주 잘하기 때문입니다. BERT로 사전학습모델(Pre-trained model)을 한 번 만들어 놓으면 이것 하나만으로 쉽게 다양한 자연어처리 문제를 풀 수 있습니다. 이전까지는 자연어처리 문제마다 다 다른 복잡한 모델이 많이 존재했고 각각 다 다른 방법으로 학습을 시켜야 했거든요. 지금은 BERT보다 낫다는 여러 후계자들에게 1등 자리를 내준것 같기도 하지만, 여전히 BERT는 현역이고 NLP의 기본이기때문에 우리는 일단 BERT의 사전학습모댈을 만들어 보기로 했습니다. 그런데말입니다 그렇게 널리 사용되는 모델이라면 이미 많은 사전학습모델이 공개되어 있지는 않을까요? 이 오픈소스의 시대에? ...네 물론 많이 있습니다. 하지만 우리에겐 아직도 BERT를 만들어야하는 몇가지 이유가 있었습니다. 우선 우리의 목적은 원래의 BERT가 사용하는 토크나이저를 우리 deeqnlp 형태소 분석으로 바꾸는 것이었습니다. BERT에 기본 탑재된 토크나이저는 인접된 문자들의 출현 빈도만 고려해서 토큰을 만들기때문에 단어들이 기본 분리되어 있는 영어같은 언어에는 잘 맞지만 의미단위 어절이 붙어있는 경우가 많은 우리나라말에 적용하면 말이 안되는(의미를 반영하지 않는) 토큰들이 잔뜩 나오게 됩니다. 그래서 우리는 토크나이저에 형태소분석을 사용해서 한글을 잘 이해하는 BERT를 만들고자 했던 것입니다. 또 하나의 이유는 BERT 모델을 0부터 만들 수 있는 경험과 기술이 다양한 목적의 사전학습모델을 만들거나 다른 모델의 사전학습모델도 만들 수 있는 바탕이 될거라 생각했기 때문입니다.
(그림?)

# CORPUS - 말뭉치

위에서 사전학습모델만 만들어두면 만사 OK인것처럼 넘어갔지만 사실은 여기 함정이 있습니다. '사전학습'의 목적이 이 세상 모든 문서를 다 읽어서 마치 언어를 이해하는 것같은 모델을 만드는 것에 있다는 겁니다. 즉 사전학습모델을 만들기 위해서는 엄청나게 많은 말뭉치가 필요합니다. 원조 BERT가 총 30억어절의 문서를 3~4번 학습했다고 하죠. 우리도 보통 수백만에서 수천만 줄의 문서를 준비해서 학습을 여러번 시켜야 합니다. 우선은 그만한 문서가 있다고 치고 학습에 맞는 문서는 어떤것이고 어떻게 준비(업계 용어로 정제라고 합니다)해야 하는지 알아봅니다.
- 우선 문법, 어법에 맞고 보통 많이 쓰이는 완결된 문장부터 학습을 시키는 것이 좋습니다. 책, 뉴스, 사전 등이 좋겠죠. 양은 최소 수백만 줄은 넘어야 하고 다양한 내용이 섞여 있을수록 좋은것 같습니다.
- 이런 원본 말뭉치들이 준비되었으면 학습데이터를 만들 수 있는 기본 말뭉치로 만듭니다. 기본 말뭉치는 text파일이고 **한 줄에 한 문장**, **문단과 문단사이는 빈 문장** 의 법칙만 지키면 됩니다!
- 이것을 위해서 준비된 원본의 포맷에 따라 text, csv, json, xml 리더 등을 만들어야 합니다.
- 문장분리, 특수 문자 제거 등의 전처리를 해야 합니다.
- 불용어, 의미가 없는 문장 삭제 등의 처리를 해야합니다. 의미 없는 내용이 포함되면 학습에 방해가 되기때문에 중요한데 이것은 소스에 따라 완전 케바케이므로 사용하려는 원본을 잘 보고 적절한 전처리를 만들어야 합니다.
- 예를들어 문장이 한단어(또는 두 단어 이하?)이면 제목이거나 의미를 알 수 없는 내용일 것이므로 삭제. 이메일, 전화번호, url등이 빈번하게 등장하는 소스라면 모두 불용어 처리해야 합니다. 이모지, 한자 등의 삭제. 반복되는 문자 삭제. 반복되는 불용어 문장(이건 뉴스, 게시판 스크래핑 데이터에 포함될 수 있는 광고, 명령 라인 등 여러 경우가 있을 수 있습니다)에 대한 패턴검색 삭제 등 엄청 다양합니다.
(그림)

# vocab - Tokenizer

vocab은 중요합니다. 자연어를 모델의 입력이 될 수 있는 숫자로 바꾸는 기준이 되는 사전이 vocab, 바꾸는 방법이 토크나이저라고 할 수 있습니다. 아무튼 일단 vocab을 만들면 vocab을 기준으로 모델이 만들어지고 이후 모델과 vocab은 같이 다녀야합니다. vocab이 없어지면 모델은 아무 의미없는 숫자덩어리입니다. 우리는 두가지 vocab을 만들고 두가지 모델을 만들겁니다. 하나는 기본 토크나이저(BERT)로 만든 vocab과 모델, 두번째는 deeqnlp로 만든 토크나이저와 vocab, 그리고 모델입니다. 기본 토크나이저에 의한 vocab과 모델은 모델 성능 평가의 기준점이 되기도하고 다른 사람들이 사용하기에, 또 우리가 여러가지 태스크에 사용하기도 좋은 기준 모델이 됩니다. BERT를 사용하는 모든 곳에 우리 모델 바이너리만 배포하면 사용할 수 있기 때문입니다. 두번째 모델은 사용하기가 간단하지 않을 수 있습니다. 우리의 사전학습모델과 함께 토크나이저의 변경이 필요하기때문입니다. (소스블럭)

# 학습데이터 생성

말뭉치를 학습을 할 수 있는 데이터로 바꿔야 합니다.

# 학습

이제 필요한건 돈입니다.

# finetune

쓸만한가요? NO

